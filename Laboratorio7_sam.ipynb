{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DirectML está configurado correctamente\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "from glob import glob\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import torch_directml\n",
    "\n",
    "# Verificar si DirectML está disponible\n",
    "if torch_directml.is_available():\n",
    "    device = torch_directml.device(0)  # Usar el primer dispositivo (si tienes una GPU compatible)\n",
    "    print(\"DirectML está configurado correctamente\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"DirectML no está disponible, usando CPU\")\n",
    "\n",
    "# Definir rutas y parámetros\n",
    "DATASET_PATH = \"Dataset-Flores\"\n",
    "CLASSES = [\"astromelia\", \"cartucho\", \"lirio\", \"obispo\", \"sanjuan\"]\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo SAM...\n",
      "Modelo SAM cargado correctamente\n"
     ]
    }
   ],
   "source": [
    "def load_sam_model():\n",
    "    try:\n",
    "        # Importar SAM\n",
    "        from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "        \n",
    "        # Verificar si ya existe el modelo descargado\n",
    "        model_path = \"sam_vit_h_4b8939.pth\"\n",
    "        if not os.path.exists(model_path):\n",
    "            print(\"Descargando modelo SAM...\")\n",
    "            import urllib.request\n",
    "            urllib.request.urlretrieve(\n",
    "                \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n",
    "                model_path\n",
    "            )\n",
    "        \n",
    "        print(\"Cargando modelo SAM...\")\n",
    "        # Usar CPU para SAM ya que DirectML podría no ser compatible directamente\n",
    "        sam_device = torch.device(\"cpu\")\n",
    "        sam = sam_model_registry[\"vit_h\"](checkpoint=model_path)\n",
    "        sam.to(sam_device)\n",
    "        \n",
    "        # Crear generador de máscaras automático\n",
    "        mask_generator = SamAutomaticMaskGenerator(\n",
    "            model=sam,\n",
    "            points_per_side=32,  # Más puntos = más segmentaciones detalladas pero más lento\n",
    "            pred_iou_thresh=0.86,  # Umbral de calidad de predicción \n",
    "            stability_score_thresh=0.92,  # Umbral de estabilidad\n",
    "            crop_n_layers=1,  # Número de capas para recortar la imagen\n",
    "            crop_n_points_downscale_factor=2,  # Factor de reducción de puntos en recortes\n",
    "            min_mask_region_area=100,  # Área mínima de segmentación en píxeles\n",
    "        )\n",
    "        \n",
    "        # Crear predictor para puntos específicos (útil si quieres guiar la segmentación)\n",
    "        predictor = SamPredictor(sam)\n",
    "        \n",
    "        print(\"Modelo SAM cargado correctamente\")\n",
    "        return mask_generator, predictor, True\n",
    "    \n",
    "    except ImportError:\n",
    "        print(\"ADVERTENCIA: No se pudo importar el módulo segment-anything.\")\n",
    "        print(\"Por favor, instala SAM con: pip install git+https://github.com/facebookresearch/segment-anything.git\")\n",
    "        return None, None, False\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error al cargar el modelo SAM: {e}\")\n",
    "        return None, None, False\n",
    "\n",
    "# Cargar modelo SAM\n",
    "sam_mask_generator, sam_predictor, sam_available = load_sam_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando dataset de entrenamiento\n",
      "Encontradas 400 imágenes en la clase astromelia\n",
      "Encontradas 400 imágenes en la clase cartucho\n",
      "Encontradas 400 imágenes en la clase lirio\n",
      "Encontradas 400 imágenes en la clase obispo\n",
      "Encontradas 400 imágenes en la clase sanjuan\n",
      "Total de imágenes encontradas: 2000\n",
      "Dataset de entrenamiento creado con 1600 imágenes\n",
      "Inicializando dataset de validación\n",
      "Encontradas 400 imágenes en la clase astromelia\n",
      "Encontradas 400 imágenes en la clase cartucho\n",
      "Encontradas 400 imágenes en la clase lirio\n",
      "Encontradas 400 imágenes en la clase obispo\n",
      "Encontradas 400 imágenes en la clase sanjuan\n",
      "Total de imágenes encontradas: 2000\n",
      "Dataset de validación creado con 400 imágenes\n",
      "Datos de entrenamiento: 1600 imágenes\n",
      "Datos de validación: 400 imágenes\n"
     ]
    }
   ],
   "source": [
    "class FloresDataset(Dataset):\n",
    "    def __init__(self, dataset_path, classes, img_size=128, transform=None, train=True, \n",
    "                 sam_mask_generator=None, cache_dir=None):\n",
    "        self.dataset_path = dataset_path\n",
    "        self.classes = classes\n",
    "        self.img_size = img_size\n",
    "        self.transform = transform\n",
    "        self.train = train\n",
    "        self.sam_mask_generator = sam_mask_generator\n",
    "        self.image_paths = []\n",
    "        \n",
    "        # Directorio para cachear máscaras generadas por SAM\n",
    "        self.cache_dir = cache_dir\n",
    "        if self.cache_dir and not os.path.exists(self.cache_dir):\n",
    "            os.makedirs(self.cache_dir, exist_ok=True)\n",
    "            print(f\"Creado directorio de caché para máscaras: {self.cache_dir}\")\n",
    "        \n",
    "        print(f\"Inicializando dataset {'de entrenamiento' if train else 'de validación'}\")\n",
    "        self.load_data()\n",
    "        \n",
    "        if len(self.image_paths) == 0:\n",
    "            raise ValueError(\"¡No se encontraron imágenes válidas! Verifica la estructura de carpetas.\")\n",
    "    \n",
    "    def load_data(self):\n",
    "        all_image_paths = []\n",
    "        \n",
    "        for class_name in self.classes:\n",
    "            class_path = os.path.join(self.dataset_path, class_name)\n",
    "            \n",
    "            # Verificar si existe la carpeta de clase\n",
    "            if not os.path.exists(class_path):\n",
    "                print(f\"ADVERTENCIA: La carpeta de clase {class_path} no existe\")\n",
    "                continue\n",
    "            \n",
    "            # Verificar subcarpeta de imágenes\n",
    "            images_path = os.path.join(class_path, \"images\")\n",
    "            if not os.path.exists(images_path):\n",
    "                print(f\"ADVERTENCIA: La carpeta de imágenes {images_path} no existe\")\n",
    "                continue\n",
    "            \n",
    "            # Obtener todas las imágenes\n",
    "            image_files = glob(os.path.join(images_path, \"*.jpg\")) + \\\n",
    "                          glob(os.path.join(images_path, \"*.jpeg\")) + \\\n",
    "                          glob(os.path.join(images_path, \"*.png\"))\n",
    "            \n",
    "            print(f\"Encontradas {len(image_files)} imágenes en la clase {class_name}\")\n",
    "            all_image_paths.extend(image_files)\n",
    "        \n",
    "        print(f\"Total de imágenes encontradas: {len(all_image_paths)}\")\n",
    "        \n",
    "        if len(all_image_paths) == 0:\n",
    "            print(\"ERROR: No se encontraron imágenes en ninguna de las clases\")\n",
    "            return\n",
    "        \n",
    "        # Dividir en entrenamiento y validación\n",
    "        indices = list(range(len(all_image_paths)))\n",
    "        np.random.seed(42)  # Para reproducibilidad\n",
    "        np.random.shuffle(indices)\n",
    "        split_idx = int(0.8 * len(indices))\n",
    "        \n",
    "        if self.train:\n",
    "            selected_indices = indices[:split_idx]\n",
    "        else:\n",
    "            selected_indices = indices[split_idx:]\n",
    "        \n",
    "        self.image_paths = [all_image_paths[i] for i in selected_indices]\n",
    "        print(f\"Dataset {'de entrenamiento' if self.train else 'de validación'} creado con {len(self.image_paths)} imágenes\")\n",
    "    \n",
    "    def generate_mask_with_sam(self, img, img_path):\n",
    "        \"\"\"Genera una máscara utilizando SAM o carga desde caché si está disponible\"\"\"\n",
    "        if self.sam_mask_generator is None:\n",
    "            # Si SAM no está disponible, crear una máscara simple de forma básica\n",
    "            # Esto es un fallback y no dará buenos resultados\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "            _, mask = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n",
    "            return mask\n",
    "        \n",
    "        # Verificar si existe en caché\n",
    "        if self.cache_dir:\n",
    "            img_name = os.path.basename(img_path)\n",
    "            cache_path = os.path.join(self.cache_dir, f\"{os.path.splitext(img_name)[0]}_mask.png\")\n",
    "            \n",
    "            if os.path.exists(cache_path):\n",
    "                # Cargar desde caché\n",
    "                mask = cv2.imread(cache_path, cv2.IMREAD_GRAYSCALE)\n",
    "                return mask\n",
    "        \n",
    "        # Generar máscara con SAM\n",
    "        masks = self.sam_mask_generator.generate(img)\n",
    "        \n",
    "        if not masks:\n",
    "            # Si SAM no genera máscaras, crear una vacía\n",
    "            final_mask = np.zeros(img.shape[:2], dtype=np.uint8)\n",
    "        else:\n",
    "            # Ordenar por área (de mayor a menor)\n",
    "            masks = sorted(masks, key=lambda x: x['area'], reverse=True)\n",
    "            \n",
    "            # Tomar la máscara más grande como la principal\n",
    "            largest_mask = masks[0]['segmentation'].astype(np.uint8) * 255\n",
    "            final_mask = largest_mask\n",
    "        \n",
    "        # Guardar en caché si corresponde\n",
    "        if self.cache_dir:\n",
    "            img_name = os.path.basename(img_path)\n",
    "            cache_path = os.path.join(self.cache_dir, f\"{os.path.splitext(img_name)[0]}_mask.png\")\n",
    "            cv2.imwrite(cache_path, final_mask)\n",
    "        \n",
    "        return final_mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        \n",
    "        # Cargar y preprocesar imagen\n",
    "        try:\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                raise ValueError(f\"No se pudo cargar la imagen: {img_path}\")\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Generar máscara con SAM o cargar desde caché\n",
    "            mask = self.generate_mask_with_sam(img, img_path)\n",
    "            \n",
    "            # Redimensionar para la red\n",
    "            img_resized = cv2.resize(img, (self.img_size, self.img_size))\n",
    "            mask_resized = cv2.resize(mask, (self.img_size, self.img_size))\n",
    "            \n",
    "            # Normalizar\n",
    "            img_normalized = img_resized / 255.0\n",
    "            mask_normalized = mask_resized / 255.0\n",
    "            \n",
    "            # Formato para PyTorch (C, H, W)\n",
    "            img_tensor = torch.from_numpy(img_normalized.transpose(2, 0, 1)).float()\n",
    "            mask_tensor = torch.from_numpy(mask_normalized).float().unsqueeze(0)  # Añadir dimensión de canal\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar la imagen {img_path}: {e}\")\n",
    "            # Crear tensores vacíos como fallback\n",
    "            img_tensor = torch.zeros((3, self.img_size, self.img_size), dtype=torch.float32)\n",
    "            mask_tensor = torch.zeros((1, self.img_size, self.img_size), dtype=torch.float32)\n",
    "        \n",
    "        # Aplicar transformaciones si existen\n",
    "        if self.transform:\n",
    "            # Crear un diccionario con ambos tensores\n",
    "            sample = {'image': img_tensor, 'mask': mask_tensor}\n",
    "            sample = self.transform(sample)\n",
    "            img_tensor, mask_tensor = sample['image'], sample['mask']\n",
    "        \n",
    "        return img_tensor, mask_tensor\n",
    "\n",
    "# Transformaciones para aumentación de datos\n",
    "class RandomTransform:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # Flip horizontal\n",
    "        if random.random() < self.p:\n",
    "            image = torch.flip(image, [2])\n",
    "            mask = torch.flip(mask, [2])\n",
    "        \n",
    "        # Flip vertical\n",
    "        if random.random() < self.p:\n",
    "            image = torch.flip(image, [1])\n",
    "            mask = torch.flip(mask, [1])\n",
    "        \n",
    "        return {'image': image, 'mask': mask}\n",
    "\n",
    "# Crear datasets\n",
    "try:\n",
    "    # Directorio de caché para máscaras SAM\n",
    "    cache_dir = \"sam_masks_cache\"\n",
    "    \n",
    "    # Crear transformaciones\n",
    "    train_transform = RandomTransform(p=0.5)\n",
    "    \n",
    "    # Crear datasets\n",
    "    train_dataset = FloresDataset(\n",
    "        DATASET_PATH, \n",
    "        CLASSES, \n",
    "        IMG_SIZE, \n",
    "        transform=train_transform, \n",
    "        train=True, \n",
    "        sam_mask_generator=sam_mask_generator,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    \n",
    "    val_dataset = FloresDataset(\n",
    "        DATASET_PATH, \n",
    "        CLASSES, \n",
    "        IMG_SIZE, \n",
    "        transform=None, \n",
    "        train=False, \n",
    "        sam_mask_generator=sam_mask_generator,\n",
    "        cache_dir=cache_dir\n",
    "    )\n",
    "    \n",
    "    # Crear dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "    \n",
    "    print(f\"Datos de entrenamiento: {len(train_dataset)} imágenes\")\n",
    "    print(f\"Datos de validación: {len(val_dataset)} imágenes\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error al crear los datasets: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de parámetros entrenables: 31,043,521\n"
     ]
    }
   ],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=1):\n",
    "        super(UNet, self).__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.down1 = DoubleConv(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.down2 = DoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.down3 = DoubleConv(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.down4 = DoubleConv(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # Bridge\n",
    "        self.bridge = DoubleConv(512, 1024)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.up_conv1 = DoubleConv(1024, 512)\n",
    "        \n",
    "        self.up2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.up_conv2 = DoubleConv(512, 256)\n",
    "        \n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.up_conv3 = DoubleConv(256, 128)\n",
    "        \n",
    "        self.up4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.up_conv4 = DoubleConv(128, 64)\n",
    "        \n",
    "        # Output\n",
    "        self.out = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.down1(x)\n",
    "        x = self.pool1(x1)\n",
    "        \n",
    "        x2 = self.down2(x)\n",
    "        x = self.pool2(x2)\n",
    "        \n",
    "        x3 = self.down3(x)\n",
    "        x = self.pool3(x3)\n",
    "        \n",
    "        x4 = self.down4(x)\n",
    "        x = self.pool4(x4)\n",
    "        \n",
    "        # Bridge\n",
    "        x = self.bridge(x)\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.up1(x)\n",
    "        x = torch.cat([x, x4], dim=1)\n",
    "        x = self.up_conv1(x)\n",
    "        \n",
    "        x = self.up2(x)\n",
    "        x = torch.cat([x, x3], dim=1)\n",
    "        x = self.up_conv2(x)\n",
    "        \n",
    "        x = self.up3(x)\n",
    "        x = torch.cat([x, x2], dim=1)\n",
    "        x = self.up_conv3(x)\n",
    "        \n",
    "        x = self.up4(x)\n",
    "        x = torch.cat([x, x1], dim=1)\n",
    "        x = self.up_conv4(x)\n",
    "        \n",
    "        # Output\n",
    "        x = self.out(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Inicializar modelo\n",
    "model = UNet(in_channels=3, out_channels=1)\n",
    "model = model.to(device)\n",
    "\n",
    "# Función para contar parámetros\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Número de parámetros entrenables: {count_parameters(model):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir funciones de pérdida y métricas\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.1, verbose=True)\n",
    "\n",
    "# Función para calcular IoU\n",
    "def calculate_iou(pred, target):\n",
    "    pred = (pred > 0.5).float()\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    if union < 1e-6:\n",
    "        return 0\n",
    "    return (intersection / union).item()\n",
    "\n",
    "# Función de entrenamiento\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    iou_scores = []\n",
    "    \n",
    "    for images, masks in tqdm(dataloader):\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Estadísticas\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calcular IoU\n",
    "        with torch.no_grad():\n",
    "            batch_iou = calculate_iou(outputs.detach(), masks)\n",
    "            iou_scores.append(batch_iou)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_iou = sum(iou_scores) / len(iou_scores) if iou_scores else 0\n",
    "    \n",
    "    return epoch_loss, epoch_iou\n",
    "\n",
    "# Función de validación\n",
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    iou_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(dataloader):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Estadísticas\n",
    "            running_loss += loss.item()\n",
    "            batch_iou = calculate_iou(outputs, masks)\n",
    "            iou_scores.append(batch_iou)\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_iou = sum(iou_scores) / len(iou_scores) if iou_scores else 0\n",
    "    \n",
    "    return epoch_loss, epoch_iou\n",
    "\n",
    "# Función para guardar resultados del entrenamiento\n",
    "def save_training_plots(train_losses, val_losses, train_ious, val_ious):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Entrenamiento')\n",
    "    plt.plot(val_losses, label='Validación')\n",
    "    plt.title('Pérdida por época')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_ious, label='Entrenamiento')\n",
    "    plt.plot(val_ious, label='Validación')\n",
    "    plt.title('IoU por época')\n",
    "    plt.xlabel('Época')\n",
    "    plt.ylabel('IoU')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_results.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=10):\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_ious = []\n",
    "    val_ious = []\n",
    "    patience = 3\n",
    "    no_improve = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Época {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        # Entrenamiento\n",
    "        train_loss, train_iou = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        train_losses.append(train_loss)\n",
    "        train_ious.append(train_iou)\n",
    "        \n",
    "        # Validación\n",
    "        val_loss, val_iou = validate(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_ious.append(val_iou)\n",
    "        \n",
    "        # Actualizar learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        print(f\"Entrenamiento - Pérdida: {train_loss:.4f}, IoU: {train_iou:.4f}\")\n",
    "        print(f\"Validación - Pérdida: {val_loss:.4f}, IoU: {val_iou:.4f}\")\n",
    "        \n",
    "        # Guardar mejor modelo\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_sam_model.pth\")\n",
    "            print(\"Guardando mejor modelo...\")\n",
    "            no_improve = 0\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        \n",
    "        # Early stopping\n",
    "        if no_improve >= patience:\n",
    "            print(f\"Early stopping después de {patience} épocas sin mejora\")\n",
    "            break\n",
    "    \n",
    "    # Guardar modelo final\n",
    "    torch.save(model.state_dict(), \"final_unet_model.pth\")\n",
    "    \n",
    "    # Guardar gráficas\n",
    "    save_training_plots(train_losses, val_losses, train_ious, val_ious)\n",
    "    \n",
    "    return train_losses, val_losses, train_ious, val_ious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(model, dataloader, num_samples=5, device=device):\n",
    "    model.eval()\n",
    "    images, masks, predictions = [], [], []\n",
    "    \n",
    "    # Obtener algunas muestras\n",
    "    with torch.no_grad():\n",
    "        for img, mask in dataloader:\n",
    "            if len(images) >= num_samples:\n",
    "                break\n",
    "            \n",
    "            img_batch = img.to(device)\n",
    "            pred_batch = model(img_batch)\n",
    "            \n",
    "            # Convertir a NumPy para visualización\n",
    "            for i in range(min(len(img), num_samples - len(images))):\n",
    "                images.append(img[i].cpu().numpy())\n",
    "                masks.append(mask[i].cpu().numpy())\n",
    "                predictions.append(pred_batch[i].cpu().numpy())\n",
    "            \n",
    "            if len(images) >= num_samples:\n",
    "                break\n",
    "    \n",
    "    # Visualizar\n",
    "    plt.figure(figsize=(15, 5 * num_samples))\n",
    "    for i in range(num_samples):\n",
    "        # Imagen original\n",
    "        plt.subplot(num_samples, 3, i*3 + 1)\n",
    "        img_display = np.transpose(images[i], (1, 2, 0))  # (C,H,W) -> (H,W,C)\n",
    "        plt.imshow(img_display)\n",
    "        plt.title(f\"Imagen {i+1}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Máscara real\n",
    "        plt.subplot(num_samples, 3, i*3 + 2)\n",
    "        plt.imshow(masks[i][0], cmap='gray')\n",
    "        plt.title(f\"Máscara (SAM) {i+1}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Máscara predicha\n",
    "        plt.subplot(num_samples, 3, i*3 + 3)\n",
    "        plt.imshow(predictions[i][0], cmap='gray')\n",
    "        plt.title(f\"Predicción U-Net {i+1}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('prediction_examples.png')\n",
    "    plt.show()\n",
    "\n",
    "# Cargar el mejor modelo y visualizar\n",
    "def load_and_visualize():\n",
    "    if os.path.exists(\"best_unet_model.pth\"):\n",
    "        model.load_state_dict(torch.load(\"best_unet_model.pth\", map_location=device))\n",
    "        print(\"Modelo cargado correctamente\")\n",
    "        \n",
    "        # Visualizar algunas predicciones\n",
    "        visualize_predictions(model, val_loader, num_samples=5, device=device)\n",
    "    else:\n",
    "        print(\"No se encontró el modelo guardado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_images(model, input_dir, output_dir=\"predicciones\", device=device):\n",
    "    model.eval()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Obtener todas las imágenes\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']\n",
    "    image_files = []\n",
    "    \n",
    "    for ext in image_extensions:\n",
    "        image_files.extend(glob(os.path.join(input_dir, f\"*{ext}\")))\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"No se encontraron imágenes en {input_dir}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Procesando {len(image_files)} imágenes...\")\n",
    "    \n",
    "    for img_path in tqdm(image_files):\n",
    "        # Cargar imagen\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"No se pudo cargar la imagen: {img_path}\")\n",
    "            continue\n",
    "            \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Guardar tamaño original\n",
    "        original_size = img.shape[:2]\n",
    "        \n",
    "        # Preprocesar\n",
    "        img_resized = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "        img_tensor = torch.from_numpy(img_resized.transpose(2, 0, 1)).float() / 255.0\n",
    "        img_tensor = img_tensor.unsqueeze(0).to(device)  # Añadir dimensión de batch\n",
    "        \n",
    "        # Predecir\n",
    "        with torch.no_grad():\n",
    "            pred_mask = model(img_tensor)\n",
    "            pred_mask = pred_mask.squeeze().cpu().numpy()\n",
    "        \n",
    "        # Umbralización para binarizar\n",
    "        pred_binary = (pred_mask > 0.5).astype(np.uint8) * 255\n",
    "        \n",
    "        # Redimensionar al tamaño original\n",
    "        pred_resized = cv2.resize(pred_binary, (original_size[1], original_size[0]))\n",
    "        \n",
    "        # Crear superposición\n",
    "        overlay = img.copy()\n",
    "        colored_mask = np.zeros_like(img)\n",
    "        colored_mask[:,:,1] = pred_resized  # Canal verde\n",
    "        \n",
    "        # Superponer máscara en imagen original\n",
    "        alpha = 0.5\n",
    "        cv2.addWeighted(colored_mask, alpha, overlay, 1 - alpha, 0, overlay)\n",
    "        \n",
    "        # Guardar resultados\n",
    "        base_name = os.path.basename(img_path).split('.')[0]\n",
    "        \n",
    "        # Guardar imagen original\n",
    "        cv2.imwrite(os.path.join(output_dir, f\"{base_name}_original.jpg\"), cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        # Guardar máscara predicha\n",
    "        cv2.imwrite(os.path.join(output_dir, f\"{base_name}_mask.png\"), pred_resized)\n",
    "        \n",
    "        # Guardar superposición\n",
    "        cv2.imwrite(os.path.join(output_dir, f\"{base_name}_overlay.jpg\"), cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    print(f\"Predicciones guardadas en {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     print(\"Iniciando proceso de segmentación de flores con U-Net y SAM en PyTorch\")\n",
    "    \n",
    "#     if not sam_available:\n",
    "#         print(\"ADVERTENCIA: SAM no está disponible. Las máscaras se generarán de forma básica.\")\n",
    "#         print(\"Se recomienda instalar SAM para obtener mejores resultados\")\n",
    "    \n",
    "#     try:\n",
    "#         # Verificar si tenemos datasets válidos\n",
    "#         if 'train_loader' not in globals() or 'val_loader' not in globals():\n",
    "#             print(\"ERROR: No se pudieron crear los dataloaders correctamente\")\n",
    "#             return\n",
    "        \n",
    "#         # Entrenar modelo\n",
    "#         print(\"\\nIniciando entrenamiento del modelo...\")\n",
    "#         train_losses, val_losses, train_ious, val_ious = train_model(\n",
    "#             model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=10\n",
    "#         )\n",
    "        \n",
    "#         # Visualizar resultados\n",
    "#         print(\"\\nVisualizando resultados...\")\n",
    "#         load_and_visualize()\n",
    "        \n",
    "#         print(\"\\nProceso completado con éxito\")\n",
    "#         print(\"Para realizar inferencia en nuevas imágenes, use la función predict_on_images()\")\n",
    "#         print(\"Ejemplo: predict_on_images('carpeta_nuevas_imagenes', 'carpeta_resultados')\")\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nERROR durante la ejecución: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "densenet_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
